{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "1. **What is the GLUE benchmark?**\n",
    "   - A) A library for fine-tuning models\n",
    "   - B) An academic benchmark for text classification tasks\n",
    "   - C) A dataset hosting platform\n",
    "   - D) A technique for caching datasets\n",
    "\n",
    "2. **How many pairs of sentences are there in the training set of the MRPC dataset?**\n",
    "   - A) 3,668\n",
    "   - B) 408\n",
    "   - C) 1,725\n",
    "   - D) 5,801\n",
    "\n",
    "3. **What does the 'label' column in the MRPC dataset represent?**\n",
    "   - A) Integer representation of sentences\n",
    "   - B) Column for sentence length\n",
    "   - C) Type of sentence structure\n",
    "   - D) Indicates whether sentences are paraphrases\n",
    "\n",
    "4. **Where is the MRPC dataset cached by default upon downloading using the ðŸ¤— Datasets library?**\n",
    "   - A) ~/.cache/huggingface/datasets\n",
    "   - B) ~/.data/huggingface/datasets\n",
    "   - C) ~/datasets/huggingface/cache\n",
    "   - D) ~/.huggingface/cache/datasets\n",
    "\n",
    "5. **What type of label corresponds to \"not_equivalent\" in the MRPC dataset?**\n",
    "   - A) 0\n",
    "   - B) 1\n",
    "   - C) ClassLabel\n",
    "   - D) 'sentence1'\n",
    "\n",
    "6. **What is the purpose of the ðŸ¤— Datasets library?**\n",
    "   - A) Simplify downloading and caching of datasets\n",
    "   - B) Model fine-tuning process overview\n",
    "   - C) Tokenizing sentences\n",
    "   - D) Structuring sentences for training\n",
    "\n",
    "7. **Which command retrieves the MRPC dataset from the Hub using the ðŸ¤— Datasets library?**\n",
    "   - A) `load_dataset('MRPC')`\n",
    "   - B) `get_dataset('MRPC')`\n",
    "   - C) `datasets.load('MRPC')`\n",
    "   - D) `load_mrpc_dataset()`\n",
    "\n",
    "8. **What does the `DatasetDict` object contain after retrieving the MRPC dataset?**\n",
    "   - A) Training set only\n",
    "   - B) Validation set only\n",
    "   - C) Training set, validation set, and test set\n",
    "   - D) Sentences without labels\n",
    "\n",
    "9. **What type of object stores the labels in the MRPC dataset?**\n",
    "   - A) Integer\n",
    "   - B) ClassLabel\n",
    "   - C) DatasetDict\n",
    "   - D) IndexedList\n",
    "\n",
    "10. **What information does examining the 'features' of the `raw_train_dataset` provide?**\n",
    "   - A) Label mapping to integers\n",
    "   - B) Length of sentences\n",
    "   - C) Number of sentences in the dataset\n",
    "   - D) Encoding of sentences into tokens\n",
    "\n",
    "11. **How does the tokenizer handle a pair of sentences, as shown in the example?**\n",
    "   - A) Tokenizes the sentences independently\n",
    "   - B) Combines both sentences into a single token sequence\n",
    "   - C) Returns a single tensor for the pair\n",
    "   - D) Creates separate dictionaries for each sentence\n",
    "   \n",
    "12. **What is the purpose of the `token_type_ids` when tokenizing a pair of sentences?**\n",
    "   - A) Separates tokens for different languages\n",
    "   - B) Indicates token positions within a sentence\n",
    "   - C) Specifies the number of tokens in a sequence\n",
    "   - D) Identifies different segments within the input pair\n",
    "   \n",
    "13. **How is dynamic padding advantageous during dataset preprocessing?**\n",
    "   - A) Decreases tokenization speed\n",
    "   - B) Reduces the size of the dataset\n",
    "   - C) Minimizes RAM usage\n",
    "   - D) Improves training efficiency by padding as needed for each batch\n",
    "   \n",
    "14. **What is the role of the `DataCollatorWithPadding` class in dataset preparation?**\n",
    "   - A) Provides token type IDs for collated samples\n",
    "   - B) Handles tokenization using multiprocessing\n",
    "   - C) Performs padding based on the longest element in a batch\n",
    "   - D) Organizes dataset samples into batches with equal lengths\n",
    "   \n",
    "15. **Why might dynamic padding cause issues when training on a TPU?**\n",
    "   - A) TPUs have limited processing power\n",
    "   - B) TPUs require fixed shapes for inputs\n",
    "   - C) TPUs prioritize token type IDs over padding\n",
    "   - D) TPUs prefer smaller batch sizes for efficiency\n",
    "   \n",
    "16. **Which method in the Dataset.map() function accelerates tokenization?**\n",
    "   - A) batched=True\n",
    "   - B) num_proc=4\n",
    "   - C) map_parallel()\n",
    "   - D) apply_to_batch()\n",
    "   \n",
    "17. **What structure does the collate function return when batching samples using the `DataCollatorWithPadding` class?**\n",
    "   - A) List of dictionaries\n",
    "   - B) Tensor of token IDs\n",
    "   - C) Dictionary of tensors\n",
    "   - D) Padding token sequence\n",
    "   \n",
    "18. **What information does the lengths of tensors in the batch represent?**\n",
    "   - A) The number of tokens in each sequence\n",
    "   - B) The number of sentences in the dataset\n",
    "   - C) The number of columns in the collated data\n",
    "   - D) The size of the vocabulary used in tokenization\n",
    "   \n",
    "19. **What does the `Dataset.map()` method do when used with `batched=True`?**\n",
    "   - A) Processes each sample sequentially\n",
    "   - B) Tokens each sample in a dataset\n",
    "   - C) Applies a function to multiple dataset elements simultaneously\n",
    "   - D) Divides the dataset into mini-batches for training\n",
    "   \n",
    "20. **What is the advantage of leveraging `Dataset.map()` with a function for preprocessing?**\n",
    "    - A) Provides direct access to dataset attributes\n",
    "    - B) Allows direct interaction with dataset files\n",
    "    - C) Performs simultaneous operations on all dataset elements\n",
    "    - D) Speeds up tokenization without requiring RAM storage\n",
    "\n",
    "21. **Why might the Trainer not provide information on model performance during training?**\n",
    "   - A) Trainer doesn't support evaluation during training by default\n",
    "   - B) Trainer only prints loss information\n",
    "   - C) Trainer doesn't allow metric computation during training\n",
    "   - D) Trainer doesn't report evaluation metrics accurately\n",
    "\n",
    "22. **What does the `predictions` field from the `predict()` method represent?**\n",
    "   - A) Logits for each element of the dataset\n",
    "   - B) Labels for the dataset elements\n",
    "   - C) Metrics computed during prediction\n",
    "   - D) Training loss values\n",
    "\n",
    "23. **What's the primary purpose of the `compute_metrics()` function in the Trainer API?**\n",
    "   - A) Evaluating model performance during training\n",
    "   - B) Computing predictions for the validation set\n",
    "   - C) Setting up the Trainer configuration\n",
    "   - D) Initializing the Trainer object\n",
    "\n",
    "24. **What kind of object does the `compute()` method in `evaluate.load()` return?**\n",
    "   - A) Model configuration\n",
    "   - B) Trained model\n",
    "   - C) Dataset object\n",
    "   - D) Metrics calculator object\n",
    "\n",
    "25. **How can one improve the Trainer to report evaluation metrics during training?**\n",
    "   - A) Define a new `evaluate()` method\n",
    "   - B) Use `evaluate()` with custom evaluation steps\n",
    "   - C) Set `evaluation_strategy` in `TrainingArguments`\n",
    "   - D) Modify the Trainer class properties\n",
    "\n",
    "26. **What's the purpose of setting the `evaluation_strategy` to \"epoch\" in `TrainingArguments`?**\n",
    "   - A) Evaluates the model at every epoch\n",
    "   - B) Enables evaluation for every training step\n",
    "   - C) Disables evaluation during training\n",
    "   - D) Defines the number of epochs for training\n",
    "\n",
    "27. **What's the significance of the F1 score reported in the Trainer's `compute_metrics()`?**\n",
    "   - A) It indicates the model's accuracy on the validation set\n",
    "   - B) It measures the model's performance on the training set\n",
    "   - C) It signifies the model's precision and recall\n",
    "   - D) It reflects the model's loss during validation\n",
    "\n",
    "28. **How can one continue training or overwrite existing checkpoints in the Trainer?**\n",
    "   - A) Use `trainer.run_continued()`\n",
    "   - B) Instantiate a new Trainer object\n",
    "   - C) Define a new model configuration\n",
    "   - D) Modify the existing Trainer's training arguments\n",
    "\n",
    "29. **What does the Trainer's `fp16` argument enable?**\n",
    "   - A) Mixed-precision training\n",
    "   - B) Single-precision training\n",
    "   - C) Multi-GPU utilization\n",
    "   - D) TPUs compatibility\n",
    "\n",
    "30. **What does the Trainer's `train()` method initiate in the fine-tuning process?**\n",
    "    - A) Evaluation of the model\n",
    "    - B) Initialization of model weights\n",
    "    - C) Fine-tuning the model on the dataset\n",
    "    - D) Tokenization of input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
