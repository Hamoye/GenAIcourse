{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "Processing the Data\n",
    "\n",
    "What's the code snippet that fine-tunes a pre-trained 'bert-base-uncased' model for sequence classification on the following different context:\n",
    "\n",
    "Sequences:\n",
    "1. 'The world is full of magic.'\n",
    "2. 'Exploration leads to discovery.'\n",
    "\n",
    "Additionally:\n",
    "- Padding and truncation are applied to the tokenized sequences.\n",
    "- Labels '0' are assigned to the sequences.\n",
    "- The loss is calculated and backpropagation is performed to optimize the model parameters using an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Existing checkpoint, tokenizer, model, and sequences\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# sequences = [\n",
    "#     \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "#     \"This course is amazing!\",\n",
    "# ]\n",
    "# batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# # Adding labels to the batch\n",
    "# batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "# optimizer = AdamW(model.parameters())\n",
    "# loss = model(**batch).loss\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "# Different context for sequences\n",
    "different_sequences = [\n",
    "    \"The world is full of magic.\",\n",
    "    \"Exploration leads to discovery.\",\n",
    "]\n",
    "different_batch = tokenizer(different_sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Adding labels to the batch for the new context\n",
    "different_batch[\"labels\"] = torch.tensor([0, 0])\n",
    "\n",
    "different_loss = model(**different_batch).loss\n",
    "different_loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Loading the data\n",
    "\n",
    "What code snippet loads the Microsoft Research Paraphrase Corpus (MRPC) dataset using the Hugging Face `datasets` library within Python?\n",
    "\n",
    "The dataset is loaded using the function `load_dataset` from the 'glue' dataset collection. The resulting dataset is stored in the variable `raw_datasets`.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "What's the code snippet that extracts and displays the first data sample from the 'test' split of the Microsoft Research Paraphrase Corpus (MRPC) dataset stored in the variable 'raw_test_dataset' obtained from the previously loaded dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_dataset = raw_datasets[\"test\"]\n",
    "raw_test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. \n",
    "Preprocessing the dataset\n",
    "\n",
    "Write a code snippet that uses a tokenizer to process the sentence pairs from the 'train' split of the Microsoft Research Paraphrase Corpus (MRPC) dataset. The code should perform tokenization, enable padding, and truncation for the sentence pairs from 'sentence1' and 'sentence2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Complete the Python function 'tokenize_function(sample)' that utilizes a tokenizer to process sentence pairs. The function should take a 'sample' as input, which is expected to contain 'sentence1' and 'sentence2' keys. Utilize the 'tokenizer' function to perform truncation on these sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "\n",
    "def tokenize_function(sample):\n",
    "    return tokenizer(sample[\"sentence1\"], sample[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "Continuing from the previous task, utilize the 'map' function from the 'datasets' library to apply the 'tokenize_function' to the entire 'raw_datasets' for the Microsoft Research Paraphrase Corpus (MRPC). The function 'tokenize_function' performs tokenization, utilizing a tokenizer on sentence pairs provided in the 'sentence1' and 'sentence2' keys within each sample. Execute this mapping operation with batching enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
