{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions And Answers:\n",
    "\n",
    "1. Which of the following is **not** an application of NLP?\n",
    "   - **Answer: B) Image Recognition**\n",
    "\n",
    "2. Which NLP task involves converting spoken language into text?\n",
    "   - **Answer: C) Speech Recognition**\n",
    "\n",
    "3. What is the primary goal of NLP?\n",
    "   - **Answer: C) Enabling computers to understand human language**\n",
    "\n",
    "4. Which framework or library is commonly used for NLP tasks?\n",
    "   - **Answer: D) NLTK**\n",
    "\n",
    "5. Which type of system uses NLP to answer questions based on a given text or knowledge base?\n",
    "   - **Answer: A) Chatbots**\n",
    "\n",
    "\n",
    "6. Which of the following best describes the challenge of resolving references like pronouns in a sentence?\n",
    "   - **Answer: D) Coreference resolution**\n",
    "\n",
    "7. What makes understanding the meaning of a sentence challenging in NLP?\n",
    "   - **Answer: D) Ambiguity and context dependency**\n",
    "\n",
    "8. Which factor makes training effective NLP models challenging due to the scarcity of high-quality data?\n",
    "   - **Answer: A) Data sparsity**\n",
    "\n",
    "9. Handling sarcasm, irony, and humor in text often relies on:\n",
    "   - **Answer: C) Context, tone, and cultural knowledge**\n",
    "\n",
    "10. What poses a significant challenge in ensuring the fairness and mitigation of biases in NLP models?\n",
    "   - **Answer: D) Inheritance and amplification of biases from training data**\n",
    "\n",
    "11. Which task can Transformers perform effectively by generating concise summaries of lengthy text documents?\n",
    "   - **Answer: C) Text Summarization**\n",
    "\n",
    "12. What role do Transformers play in assisting systems like Siri, Alexa, and chatbots?\n",
    "   - **Answer: D) Language Understanding and Generation in Conversations**\n",
    "\n",
    "13. Transformers contribute significantly to machine translation systems like Google Translate by:\n",
    "   - **Answer: C) Enabling the translation of text across languages**\n",
    "\n",
    "14. Which task involves Transformers collaborating with computer vision models to generate textual descriptions for images?\n",
    "   - **Answer: C) Image Captioning**\n",
    "\n",
    "15. Transformers excel in which task, identifying and categorizing entities such as people, organizations, and locations in text?\n",
    "   - **Answer: B) Named Entity Recognition (NER)**\n",
    "\n",
    "Of course! Here are the answers corresponding to the questions provided:\n",
    "\n",
    "16. What is the primary purpose of using pipelines in NLP?\n",
    "   - **Answer: A) To define the sequence of NLP tasks**\n",
    "\n",
    "17. Which step involves configuring a sequence of NLP tasks such as tokenization, named entity recognition, and sentiment analysis?\n",
    "   - **Answer: B) Defining the Pipeline**\n",
    "\n",
    "18. After defining the pipeline, what does instantiating the pipeline involve?\n",
    "   - **Answer: C) Configuring the sequence of NLP operations**\n",
    "\n",
    "19. Which step involves running the predefined NLP tasks on text data, handling intermediate results?\n",
    "   - **Answer: C) Process Data**\n",
    "\n",
    "20. What does post-processing in the context of working with NLP pipelines entail?\n",
    "   - **Answer: C) Aggregating information or integrating results**\n",
    "\n",
    "21. What component of the Transformer architecture addresses the lack of inherent token order in sequences?\n",
    "   - **Answer: B) Positional Encoding**\n",
    "\n",
    "22. Which mechanism allows each token to consider relationships and dependencies between all other tokens in a sequence?\n",
    "   - **Answer: A) Self-Attention Mechanism**\n",
    "\n",
    "23. In Transformer architecture, what is responsible for capturing the semantic meaning of tokens in a sequence?\n",
    "   - **Answer: C) Input Embedding**\n",
    "\n",
    "24. What purpose do residual connections and layer normalization serve in Transformers?\n",
    "   - **Answer: B) Facilitating training of deeper networks and improving gradient flow**\n",
    "\n",
    "25. What is the primary function of the final layer in the Transformer architecture?\n",
    "   - **Answer: C) Producing the model's predictions**\n",
    "\n",
    "26. Which mechanism allows encoder models to capture dependencies and context effectively by enabling each token to attend to all other tokens in the sequence?\n",
    "   - **Answer: C) Self-Attention Mechanism**\n",
    "\n",
    "27. What is the primary focus of encoder models in the context of natural language processing (NLP) tasks?\n",
    "   - **Answer: C) Encoding and understanding input data**\n",
    "\n",
    "28. Which component of the encoder model is responsible for transforming each token into a fixed-dimensional vector representation?\n",
    "   - **Answer: D) Input Embedding**\n",
    "\n",
    "29. How do encoder models typically update token embeddings to produce contextual representations?\n",
    "   - **Answer: C) By capturing token relationships with other tokens**\n",
    "\n",
    "30. What is a notable characteristic of encoder models that contributes to their adaptability for various NLP tasks?\n",
    "   - **Answer: C) Contextual representations from input sequences**\n",
    "\n",
    "31. Which mechanism in decoder models allows them to focus on different parts of the input context or previously generated tokens when generating the current token?\n",
    "   - **Answer: C) Attention Mechanisms**\n",
    "\n",
    "32. In the context of natural language processing (NLP) tasks, what is the primary task of a decoder model?\n",
    "   - **Answer: B) Generating sequential output**\n",
    "\n",
    "33. What role does the context or \"thought vector\" play in the operation of a decoder model?\n",
    "   - **Answer: D) Providing relevant information from input data**\n",
    "\n",
    "34. Which feature of decoder models allows them to evolve the hidden state or context with each generated token in autoregressive generation?\n",
    "   - **Answer: B) Attention Mechanisms**\n",
    "\n",
    "35. What is a notable characteristic of decoder models that contributes to their versatility in generating structured sequences?\n",
    "   - **Answer: A) Conditional generation**\n",
    "\n",
    "36. What is one of the potential issues caused by training transformer models on large datasets?\n",
    "   - **Answer: C) Data bias, including perpetuating existing biases**\n",
    "\n",
    "37. Which challenge arises when transformer models encounter significantly different data from what they were trained on?\n",
    "   - **Answer: D) Struggle with out-of-distribution data**\n",
    "\n",
    "38. What is a strategy to address bias in transformer models during training?\n",
    "   - **Answer: C) Fairness metrics and debiasing training data**\n",
    "\n",
    "39. What potential issue can arise due to the training data containing inaccuracies in transformer models?\n",
    "   - **Answer: D) Misinformation and misleading outputs**\n",
    "\n",
    "40. Which factor contributes to the computational intensity of training and deploying transformer models?\n",
    "   - **Answer: D) Requirement of substantial computational resources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the scores (in 4 decimal places) for the labels in the code below?\n",
    "\n",
    "```python\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Text to classify\n",
    "text_to_classify = \"The latest scientific research suggests a breakthrough in renewable energy sources.\"\n",
    "\n",
    "# Candidate labels/categories\n",
    "candidate_labels = [\"technology\", \"environment\", \"health\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(text_to_classify, candidate_labels)\n",
    "\n",
    "print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why might the following code not work when using the fill-mask pipeline from the Transformers library?\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the fill-mask pipeline\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "\n",
    "# Text with a mask to fill\n",
    "text_with_mask = \"Exploring the depths of is an exciting adventure.\"\n",
    "\n",
    "# Perform the masked language modeling\n",
    "filled_mask = unmasker(text_with_mask, top_k=3)\n",
    "```\n",
    "\n",
    "### ANSWER\n",
    "\n",
    "* A) The text provided for filling the mask does not contain a valid mask token (\"\\<mask\\>\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the result of the below code?\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the Named Entity Recognition (NER) pipeline\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "# Text for named entity recognition\n",
    "new_text_to_analyze = \"During the space mission, Neil Armstrong landed on the moon in 1969.\"\n",
    "\n",
    "# Perform Named Entity Recognition (NER)\n",
    "new_ner_result = ner(new_text_to_analyze)\n",
    "\n",
    "print(new_ner_result)\n",
    "```\n",
    "### ANSWER\n",
    "* C) {'entity_group': 'PER', 'score': 0.99813056, 'word': 'Neil Armstrong', 'start': 26, 'end': 40}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
